success_1000 = success_ranked[1:1000,] #define the "successful" projects with the most backers as the most unsucessful ones
unsuccessdata = kickdata[which(kickdata$state=='failed'),]
unsuccess_ranked <- unsuccessdata %>%
arrange(.$backers_count)
unsuccess_1000 = unsuccess_ranked[1:1000,] #define the "failed" projects with the least backers as the most unsucessful ones
# merge the 1000 most successful and 1000 most unsuccessful projects together
data2000 = rbind(success_1000, unsuccess_1000)
data2000_blurb = data2000[,"blurb"]
# add doc_id
IDdf = as.data.frame(c(1:2000))
text2000 = cbind(IDdf,data2000_blurb)
library(plyr)
colnames(text2000) = c("doc_id", "text")
df2000_source = DataframeSource(text2000)
# Convert df_source to a corpus: df2000_corpus
df2000_corpus <- VCorpus(df2000_source)
# Define clean_corpus function
removeNumPunct <- function(x){gsub("[^[:alpha:][:space:]]*", "", x)}
clean_corpus <- function(corpus){
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeWords, c(stopwords("en")))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(removeNumPunct))
corpus <- tm_map(corpus, stripWhitespace)
return(corpus)
}
# Apply the function to df2000_corpus
df2000_clean <- clean_corpus(df2000_corpus)
# Stem the words left
library(SnowballC)
text2000_stemmed = tm_map(df2000_clean, stemDocument)
# Complete the stems
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
# # Oddly, stemCompletion completes an empty string to
# a word in dictionary. Remove empty string to avoid issue.
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}
library(parallel)
library(pbapply)
install.packages("pbapply")
library(pbapply)
# Calculate the number of cores
no_cores <- detectCores() - 1
# Initiate cluster
text2000_comp_all <- mclapply(text2000_stemmed,
stemCompletion2,
dictionary = df2000_clean,
mc.cores=no_cores)
# Initiate cluster
text2000_comp_all <- mclapply(text2000_stemmed,
stemCompletion2,
dictionary = df2000_clean,
mc.cores=no_cores)
text2000_comp_all <- as.VCorpus(text2000_comp_all)
text2000_dtm <- DocumentTermMatrix(text2000_comp_all)
text2000_dtm
# Convert text2000_dtm to a matrix: text2000_m
text2000_m = as.matrix(text2000_dtm)
dim(text2000_m)
text2000_m[1:3,100:103]
# Make a TDM for successful projects
# extract "blurb"
data1000_blurb = success_1000[,"blurb"]
# add doc_id
IDdf2 = as.data.frame(c(1:1000))
text1000 = cbind(IDdf2,data1000_blurb)
library(plyr)
colnames(text1000) = c("doc_id", "text")
df1000_source = DataframeSource(text1000)
# Convert df_source to a corpus: df1000_corpus
df1000_corpus <- VCorpus(df1000_source)
# Apply the clean function to df1000_corpus
df1000_clean <- clean_corpus(df1000_corpus)
# Apply the clean function to df1000_corpus
df1000_clean <- clean_corpus(df1000_corpus)
# Stem the words left
text1000_stemmed = tm_map(df1000_clean, stemDocument)
# Complete the stems, use stemCompletion2 again
text1000_comp_all <- mclapply(text1000_stemmed,
stemCompletion2,
dictionary = df2000_clean,
mc.cores=no_cores)
# Apply the clean function to df1000_corpus
df1000_clean <- clean_corpus(df1000_corpus)
# Stem the words left
text1000_stemmed = tm_map(df1000_clean, stemDocument)
# Complete the stems, use stemCompletion2 again
text1000_comp_all <- mclapply(text1000_stemmed,
stemCompletion2,
dictionary = df2000_clean,
mc.cores=no_cores)
text1000_comp_all <- as.VCorpus(text1000_comp_all)
text1000_tdm = TermDocumentMatrix(text1000_comp_all)
text1000_td = tidy(text1000_tdm)
library(dplyr)
text1000_td_df1 = as_data_frame(text1000_td)
text1000_td_df1 = as_tibble(text1000_td)
library(tidytext)
text1000_n = text1000_td_df1 %>%
group_by(term) %>%
summarise(n = sum(count)) %>%
top_n(n = 50, wt = n) %>%
mutate(term = reorder(term, n))
text1000_td_df1
View(text1000_td_df1)
text1000_td_df1 %>%
group_by(term)
text1000_n = text1000_td_df1 %>%
group_by(term)
View(text1000_n)
View(text1000_td_df1)
text1000_n = text1000_td_df1 %>%
group_by(term) %>%
summarise(n = sum(count))
View(text1000_n)
View(text1000_n)
by_cyl <- mtcars %>% group_by(cyl)
# grouping doesn't change how the data looks (apart from listing
# how it's grouped):
by_cyl
# It changes how it acts with the other dplyr verbs:
by_cyl %>% summarise(
disp = mean(disp),
hp = mean(hp)
)
text1000_n = text1000_td_df1 %>%
group_by(term) %>%
summarise(n = sum(count))
View(text1000_n)
by_cyl$cyl<-as.character(by_cyl$cyl)
by_cyl
by_cyl %>% summarise(
+     disp = mean(disp),
+     hp = mean(hp)
+ )
by_cyl %>% summarise(
disp = mean(disp),
hp = mean(hp)
)
by_cyl %>%group_by(cyl)%>% summarise(
disp = mean(disp),
hp = mean(hp)
)
by_cyl %>%group_by(cyl) %>% summarise(
disp = mean(disp)
)
text1000_n = text1000_td_df1 %>%
dyplr::group_by(term) %>%
summarise(n = sum(count))
by_cyl %>%group_by(cyl)%>% summarise(
disp = mean(disp),
hp = mean(hp)
)
library(dplyr)
text1000_n = text1000_td_df1 %>%
dyplr::group_by(term) %>%
summarise(n = sum(count))
text1000_n = text1000_td_df1 %>%
dplyr::group_by(term) %>%
summarise(n = sum(count))
text1000_n = text1000_td_df1 %>%
dplyr::group_by(term) %>%
dplyr::summarise(n = sum(count))
View(text1000_n)
text1000_n = text1000_td_df1 %>%
dplyr::group_by(term) %>%
dplyr::summarise(n = sum(count)) %>%
top_n(n = 50, wt = n) %>%
mutate(term = reorder(term, n))
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
pacman::p_load(knitr, readr, stringr, tesseract, vecsets)
source('../lib/ifCleanToken.R')
file_name_vec <- list.files("../data/ground_truth") #100 files in total
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
pacman::p_load(knitr, readr, stringr, tesseract, vecsets)
source('../lib/ifCleanToken.R')
file_name_vec <- list.files("../data/ground_truth") #100 files in total
for(i in c(1:length(file_name_vec))){
current_file_name <- sub(".txt","",file_name_vec[i])
## png folder is not provided on github (the code is only for demonstration purpose)
current_tesseract_txt <- tesseract::ocr(paste("../data/png/",current_file_name,".png",sep=""))
### clean the tessetact text (separate line by "\n", delete null string, transter to lower case)
clean_tesseract_txt <- strsplit(current_tesseract_txt,"\n")[[1]]
clean_tesseract_txt <- clean_tesseract_txt[clean_tesseract_txt!=""]
### save tesseract text file
writeLines(clean_tesseract_txt, paste("../data/tesseract/",current_file_name,".txt",sep=""))
}
setwd("/Users/xiaoxi/Documents/GitHub/Spring2019-Proj4-group10/doc")
for(i in c(1:length(file_name_vec))){
current_file_name <- sub(".txt","",file_name_vec[i])
## png folder is not provided on github (the code is only for demonstration purpose)
current_tesseract_txt <- tesseract::ocr(paste("../data/png/",current_file_name,".png",sep=""))
### clean the tessetact text (separate line by "\n", delete null string, transter to lower case)
clean_tesseract_txt <- strsplit(current_tesseract_txt,"\n")[[1]]
clean_tesseract_txt <- clean_tesseract_txt[clean_tesseract_txt!=""]
### save tesseract text file
writeLines(clean_tesseract_txt, paste("../data/tesseract/",current_file_name,".txt",sep=""))
}
getwd
getwd()
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
pacman::p_load(knitr, readr, stringr, tesseract, vecsets)
source('../lib/ifCleanToken.R')
file_name_vec <- list.files("../data/ground_truth") #100 files in total
?gsub
?parse
## in order to accelerate the computation, conduct ealy stopping
rule_list <- c("str_count(cur_token, pattern = '[A-Za-z0-9]') <= 0.5*nchar(cur_token)", # If the number of punctuation characters in a string is greater than the number of alphanumeric characters, it is garbage
"length(unique(strsplit(gsub('[A-Za-z0-9]','',substr(cur_token, 2, nchar(cur_token)-1)),'')[[1]]))>1", #Ignoring the first and last characters in a string, if there are two or more different punctuation characters in thestring, it is garbage
"nchar(cur_token)>20") #A string composed of more than 20 symbols is garbage
parse(text = rule_list[now])
parse(text = rule_list[1])
text = rule_list[1]
rule_list[1]
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
pacman::p_load(knitr, readr, stringr, tesseract, vecsets)
source('../lib/ifCleanToken.R')
file_name_vec <- list.files("../data/ground_truth") #100 files in total
for(i in c(1:length(file_name_vec))){
current_file_name <- sub(".txt","",file_name_vec[i])
## png folder is not provided on github (the code is only for demonstration purpose)
current_tesseract_txt <- tesseract::ocr(paste("../data/png/",current_file_name,".png",sep=""))
### clean the tessetact text (separate line by "\n", delete null string, transter to lower case)
clean_tesseract_txt <- strsplit(current_tesseract_txt,"\n")[[1]]
clean_tesseract_txt <- clean_tesseract_txt[clean_tesseract_txt!=""]
### save tesseract text file
writeLines(clean_tesseract_txt, paste("../data/tesseract/",current_file_name,".txt",sep=""))
}
getwd()
library(stringr)
word="asd"
str_length(word)
vowels <- c("a", "e", "i", "o", "u")
word_lower <- tolower(strsplit(word, "")[[1]])
word_freq <- word_lower[word_lower %in% letters]
letters
word_table <- table(word_freq)
word_table
l <- sum(word_table)
v <- sum(word_table[vowels], na.rm=TRUE)
strsplit(word, "")
word[2]
word_lower
word="$u%itftrf%kmaaa33"
word_lower <- tolower(strsplit(word, "")[[1]])
word_lower
word="$u%itftrf%kmaaa33ASDFSDF"
word_lower <- tolower(strsplit(word, "")[[1]])
word_lower
word_lower <- tolower(strsplit(word, "")[[1]])
word_freq <- word_lower[word_lower %in% letters]
word_freq
length(grep("[^[:alnum:]=\\.]", strsplit(word,"")[[1]]))
strsplit(word,"")
s<-length(grep("[^[:alnum:]=\\.]", strsplit(word,"")[[1]]))
feature3 <- function(word) {
s<-length(grep("[^[:alnum:]=\\.]", strsplit(word,"")[[1]]))
l<-str_length(word)
return(c(s,s/l))
}
feature3(word)
d<-length(grep("[0-9]", strsplit(word,"")[[1]]))
feature4 <- function(word){
d<-length(grep("[0-9]", strsplit(word,"")[[1]]))
l<-str_length(word)
return(c(d,d/l))
}
feature4(word)
feature5 <- function(word){
low<-length(grep("[a-z]", strsplit(word,"")[[1]]))
upp<-length(grep("[A-Z]", strsplit(word,"")[[1]]))
return(c(low,upp,low/l,upp/l))
}
feature5 <- function(word){
low<-length(grep("[a-z]", strsplit(word,"")[[1]]))
upp<-length(grep("[A-Z]", strsplit(word,"")[[1]]))
l<-str_length(word)
return(c(low,upp,low/l,upp/l))
}
feature5(word)
word="aaaaaaadfgadg"
feature5(word)
low<-length(grep("[.]{3+}", strsplit(word,"")[[1]]))
low
low<-length(grep(".{3+}", strsplit(word,"")[[1]]))
low
low<-length(grep(".{3+}", strsplit(word,"")[[1]]))
low
grep(".{3}", strsplit(word,"")[[1]])
strsplit(word,"")[[1]])
strsplit(word,"")[[1]]
grep(".{3}", word)
regmatches(word, gregexpr(".{3+}", word))
gregexpr(".{3+}", word)
gregexpr(".{3}", word)
regmatches(word, gregexpr(".{3}", word))
l<-str_length(word)
regmatches(word, gregexpr(".{3,l}", word))
regmatches(word, gregexpr(".{3,4}", word))
regmatches(word, gregexpr(".{3,}", word))
str_count(word,".{3,}")
low <- str_count(word,"[:lower:]")
low
regmatches(word, gregexpr("./1{3,}", word))
gregexpr("./1{3,}", word)
gregexpr(".\1{3,}", word)
gregexpr(".{3,}", word)
i=1
substr(word,i,i)
while (TRUE) {
cur_letter <- substr(word,i,i)
cur_count[i] <- 1
j <- i + 1
if(j==l+1){
break
}
while (substr(word,j,j)==cur_letter) {
cur_count[i] <- cur_count[i] + 1
j <- j + 1
}
i<-j
if(i==l+1){
break
}
}
cur_count <- c()
l<-str_length(word)
word
while (TRUE) {
cur_letter <- substr(word,i,i)
cur_count[i] <- 1
j <- i + 1
if(j==l+1){
break
}
while (substr(word,j,j)==cur_letter) {
cur_count[i] <- cur_count[i] + 1
j <- j + 1
}
i<-j
if(i==l+1){
break
}
}
cur_count
count <- max(cur_count)
count
count <- max(cur_count,na.rm = T)
count
feature6 <- function(word){
l<-str_length(word)
cur_count <- c()
while (TRUE) {
cur_letter <- substr(word,i,i)
cur_count[i] <- 1
j <- i + 1
if(j==l+1){
break
}
while (substr(word,j,j)==cur_letter) {
cur_count[i] <- cur_count[i] + 1
j <- j + 1
}
i<-j
if(i==l+1){
break
}
}
count <- max(cur_count,na.rm = T)
if(count<=3){
return(0)
}else{
return(count/l)
}
}
feature6(word)
words
word="aaaaaaaadsf adsf "
word="aaaaaaaadsf124sdfg  "
word="aaaaaaaadsf124sdfg"
feature6(word)
l<-str_length(word)
cur_count <- c()
while (TRUE) {
cur_letter <- substr(word,i,i)
cur_count[i] <- 1
j <- i + 1
if(j==l+1){
break
}
while (substr(word,j,j)==cur_letter) {
cur_count[i] <- cur_count[i] + 1
j <- j + 1
}
i<-j
if(i==l+1){
break
}
}
count <- max(cur_count,na.rm = T)
count
cur_count
feature6 <- function(word){
l<-str_length(word)
cur_count <- c()
i<-1
while (TRUE) {
cur_letter <- substr(word,i,i)
cur_count[i] <- 1
j <- i + 1
if(j==l+1){
break
}
while (substr(word,j,j)==cur_letter) {
cur_count[i] <- cur_count[i] + 1
j <- j + 1
}
i<-j
if(i==l+1){
break
}
}
count <- max(cur_count,na.rm = T)
if(count<=3){
return(0)
}else{
return(count/l)
}
}
feature6(word)
l_alpha<-length(grep("[[:alnum:]=\\.]", strsplit(word,"")[[1]]))
l<-nchar(word)
l_alpha
l
word="$u%itftrf%km"
l_alpha<-length(grep("[[:alnum:]=\\.]", strsplit(word,"")[[1]]))
l<-nchar(word)
l_alpha
l<-nchar(word)
l
feature7<- function(word){
l_alpha<-length(grep("[[:alnum:]=\\.]", strsplit(word,"")[[1]]))
l<-nchar(word)
k<-l-l_alpha
return(ifelse(k>l_alpha,1,0))
}
feature7(words)
feature7(word)
word="$$$$s"
feature7(word)
consec_con<-str_count(word,"[bcdfghjklmnpqrstvxzwyBCDFGHJKLMNPQRSTVXZWY]{6,}")
consec_con
word="bbbbbbbbbbb029104"
consec_con<-str_count(word,"[bcdfghjklmnpqrstvxzwyBCDFGHJKLMNPQRSTVXZWY]{6,}")
consec_con
word="bbbbbbbbbbb029104ppppppppp"
consec_con<-str_count(word,"[bcdfghjklmnpqrstvxzwyBCDFGHJKLMNPQRSTVXZWY]{6,}")
word="bbbbbbbbbbb029104ppppppppp"
consec_con
feature8<- function(word){
consec_con<-str_count(word,"[bcdfghjklmnpqrstvxzwyBCDFGHJKLMNPQRSTVXZWY]{6,}")
return(ifelse(consec_con>=1,1,0))
}
feature8(word)
feature8("adsgf awjknascj")
feature9<- function(word){
l<-nchar(word)
trim_word <- substr(word,2,(l-1))
non_alnum<-str_count(trim_word,"[^[:alnum:]]")
return(ifelse(non_alnum>=1,1,0))
}
word="sad5%%sdfmkmk"
feature9(word)
word="sad5%sdfmkmk"
feature9(word)
l<-nchar(word)
trim_word <- substr(word,2,(l-1))
trim_word
non_alnum<-str_count(trim_word,"[^[:alnum:]]")
str_count(trim_word,"[^[:alnum:]]")
ifelse(non_alnum>=2,1,0)
feature9<- function(word){
l<-nchar(word)
trim_word <- substr(word,2,(l-1))
non_alnum<-str_count(trim_word,"[^[:alnum:]]")
return(ifelse(non_alnum>=2,1,0))
}
feature9(word)
