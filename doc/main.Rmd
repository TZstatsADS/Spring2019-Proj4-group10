---
title: 'Group 10: Optical character recognition (OCR) + Error Detection + Error Correction'
author: 
- Seungwook Han
- Hongye Jiang
- Jingyue Li
- Xinzhu Wang
- Xiaoxi Zhao
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: false
    number_sections: true
    code_folding: hide
---

GU4243/GR5243: Applied Data Science

<style type="text/css">
h1.title {
  font-size: 24px;
  color: Black;
}
h1 { /* Header 1 */
  font-size: 24px;
  color: Black;
}
h2 { /* Header 2 */
  font-size: 20px;
  color: Black;
}
h3 { /* Header 3 */
  font-size: 16px;
  color: Black;
}
h4 { /* Header 4 */
  font-size: 14px;
  color: Grey;
}
</style>
# Introduction {-}

Optical character recognition (OCR) is the process of converting scanned images of machine printed or
handwritten text (numerals, letters, and symbols), into machine readable character streams, plain (e.g. text files) or formatted (e.g. HTML files). As shown in Figure 1, the data *workflow* in a typical OCR system consists of three major stages:

* Pre-processing

* Word recognition

* Post-processing

![](../figs/ocr_flowchart.png) 

We have processed raw scanned images through the first two steps are relying on the [Tessearct OCR machine](https://en.wikipedia.org/wiki/Tesseract_(software)). R package tutorial can be found [here](https://www.r-bloggers.com/the-new-tesseract-package-high-quality-ocr-in-r/). 

BUT this is not the FOCUS of this project!!!

In this project, we are going to **focus on the third stage -- post-processing**, which includes two tasks: *error detection* and *error correction*.  

# Step 1 - Load library and source code
```{r, warning=FALSE, message = FALSE}
if (!require("devtools")) install.packages("devtools")
if (!require("caret")) install.packages("caret")
if (!require("modeest")) install.packages("modeest")
if (!require("pacman")) {
  ## devtools is required
  library(devtools)
  #install_github("trinker/pacman")
}
if (!requireNamespace("BiocManager", quietly = TRUE))
  install.packages("BiocManager")
BiocManager::install("genefilter", version = "3.8")
library(genefilter)
library(e1071)
pacman::p_load(knitr, readr, stringr, tesseract, vecsets)
source('../lib/ifCleanToken.R')
source('../lib/correct_index_func1.R')
load('../output/bigram.RData')
load("../output/svm.model.pred.rda")
file_name_vec <- list.files("../data/ground_truth") #100 files in total
```

# Step 2 - Read the files and conduct Tesseract OCR

We read in the data and split the data into test data and training data by 80% and 20%. We want to train our detection and correction model on the training set and test our results on the test set.

# Step 3 - Error detection

Now, we are ready to conduct post-processing, based on the Tessearct OCR output. First of all, we need to detect errors, or *incorrectly processed words* -- check to see if an input string is a valid dictionary word or if its n-grams are all legal.

The referenced paper is D3. Probabilistic techniques -- [SVM garbage detection](https://dl.acm.org/citation.cfm?doid=2034617.2034626)

We split the text files into words and detect each word is a garbage(error) or not. We extract several features from the word to realize this: the length of string, number of vowels and consonants and their proportions, number and quotient of special(non-alphanumerical) symbol, number and quotient of digits, number and quotients of lowercase and uppercase letters, the length of maximal sequence of identical letters, the quotient of alphanumerical symbols, number of consecutive consonants, number of non-alpha-numerical sumbols in the trimmed word, bigram, most frequent symbol and non-alphabetical symbols. 

Then after extracting the features for all the words, we also label each word if they are correct by comparing them to the groundtruth file. 

After we have the features and the response, we implemnted the SVM model with RBF kernel. We tried to run the cross validation but the running time for svm model is too long so we decide to only split the data into training set and validation set to report the validation error. The final parameter chosen is Cost=1 and Gamma=1.

```{r}
#see create_matrix.R and svm.R

#performance of SVM model
load('../output/datamatrix_new.RData')
load('../output/tes_split_list.RData')

#split the dataset into training and test data 
set.seed(1)
#head(datamatrix)
s<-sample(1:2,length(file_name_vec),prob=c(0.8,0.2),replace=T)
train.index<-which(s==1)
test.index<-which(s==2)
datamatrix<-as.data.frame(datamatrix)
datamatrix$ifwordcorrect<-factor(datamatrix$ifwordcorrect)
trainset<-as.data.frame(datamatrix[datamatrix$j %in% train.index,c(-1,-2,-3,-4,-27)])
testset<-as.data.frame(datamatrix[datamatrix$j %in% test.index,c(-1,-2,-3,-4,-27)])
#report training accuracy
load("../output/svm.best.rda")
load("../output/predvalue.rda")

#report training confusion matrix
```

Our confusion matrix(including specificity and sensiticity(precision)), recall and F-1 score is listed below. 

```{r}
library(caret)
confusionMatrix(data=predvalue,reference=testset$ifwordcorrect,positive="0")->svm.table
svm.table
#recall
svm.table$table[1,1]/(svm.table$table[1,1]+svm.table$table[1,2])->recall
print(paste('recall:', recall))
#F-1 score
precision<-svm.table$table[1,1]/(svm.table$table[2,1]+svm.table$table[1,1])
f1<-2*precision*recall/(precision+recall)
print(paste('f1:', f1))
```

# Step 4 - Error correction

Given the detected word error, in order to find the best correction, we need to generating the candidate corrections: a dictionary or a database of legal n-grams to locate one or more potential correction terms. Then we need invoke some lexical-similarity measure between the misspelled string and the candidates or a probabilistic estimate of the likelihood of the correction to rank order the candidates.

Our referenced paper is [Letter n-gram](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1672564}{positional binary digram)

The C1 paper presents an error correction method for the case in which a word has 1 error -- using digram features. What's more, we extended the error correction method further by implementing an error correction method for the case in which a word has 2 errors. 

```{r}
#see correction.R

test_df <- as.data.frame(datamatrix[datamatrix[,'j'] %in% test.index,])
test_vec <- tes_split_list[datamatrix[,'j'] %in% test.index]
test_if_clean <- predvalue
#length(tesseract_if_clean)
test_if_error <- ifelse(test_if_clean==1,F,T)
#extract error terms
test_error_vec <- test_vec[test_if_error]
#Correcting words with 1 error
load("../output/aftercorrection1.RData")
#Correcting words with 2 errors
load("../output/aftercorrection2.RData")
```

# Step 5 - Performance measure

The two most common OCR accuracy measures are precision and recall. Both are relative measures of the OCR accuracy because they are computed as ratios of the correct output to the total output (precision) or input (recall). More formally defined,
\begin{align*}
\mbox{precision}&=\frac{\mbox{number of correct items}}{\mbox{number of items in OCR output}}\\
\mbox{recall}&=\frac{\mbox{number of correct items}}{\mbox{number of items in ground truth}}
\end{align*}
where *items* refer to either characters or words, and ground truth is the original text stored in the plain text file. 

Both *precision* and *recall* are mathematically convenient measures because their numeric values are some decimal fractions in the range between 0.0 and 1.0, and thus can be written as percentages. For instance, recall is the percentage of words in the original text correctly found by the OCR engine, whereas precision is the percentage of correctly found words with respect to the total word count of the OCR output. Note that in the OCR-related literature, the term OCR accuracy often refers to recall.


```{r, warning=FALSE}
#create a performance table
OCR_performance_table <- data.frame("Tesseract" = rep(NA,4),
                                    "Tesseract_with_postprocessing" = rep(NA,4))
row.names(OCR_performance_table) <- c("word_wise_recall","word_wise_precision",
                                      "character_wise_recall","character_wise_precision")

library(tidyr)
#Collecting ground truth vector
ground_truth_vec <- c()
for(i in test.index){
  inputtxt <- readLines(paste("../data/ground_truth/",file_name_vec[i],sep=""), warn=FALSE)
  inputtxt1 <- unlist(strsplit(tolower(inputtxt), " "))
  ground_truth_vec <- c(ground_truth_vec, inputtxt1)
}

# Collecting tesseract vector
tesseract_vec <- test_vec

# Loading in tesseract words after detection + correction algorithm

# Correcting words with 2 errors, this one has better performance
load("../output/aftercorrection2.RData")
aftercorrection_vec <- test_df_correct2[,2]

tesseract_vec2 <- test_df_correct2[,1]
tesseract_delete_error_vec2 <- test_df_correct2[,2]

old_intersect_vec2 <- vecsets::vintersect(tolower(ground_truth_vec), tolower(tesseract_vec2)) #607
new_intersect_vec2 <- vecsets::vintersect(tolower(ground_truth_vec), tolower(tesseract_delete_error_vec2))

OCR_performance_table["word_wise_recall","Tesseract"] <- length(old_intersect_vec2)/length(ground_truth_vec)
OCR_performance_table["word_wise_precision","Tesseract"] <- length(old_intersect_vec2)/length(tesseract_vec2)
OCR_performance_table["word_wise_recall","Tesseract_with_postprocessing"] <- length(new_intersect_vec2)/length(ground_truth_vec)
OCR_performance_table["word_wise_precision","Tesseract_with_postprocessing"] <- length(new_intersect_vec2)/length(tesseract_delete_error_vec2)
          
ground_char_count <- NULL
tess_char_count <- NULL
corrected_char_count <- NULL

## Evaluating character-level metrics (precision & recall)
for (word in ground_truth_vec) {
  word_alph <- gsub("[^[:alpha:]]+", "", word)
  char_table <- table(factor(unlist(strsplit(word_alph, ""), use.names=FALSE), levels=letters))
  if(is.null(ground_char_count)) {
    ground_char_count <- char_table
  }
  else {
    ground_char_count <- ground_char_count + char_table
  }
}

for (word in tesseract_vec) {
  word_alph <- gsub("[^[:alpha:]]+", "", word)
  char_table <- table(factor(unlist(strsplit(word_alph, ""), use.names=FALSE), levels=letters))
  if(is.null(tess_char_count)) {
    tess_char_count <- char_table
  }
  else {
    tess_char_count <- tess_char_count + char_table
  }
}

for (word in aftercorrection_vec) {
  word_alph <- gsub("[^[:alpha:]]+", "", word)
  char_table <- table(factor(unlist(strsplit(word_alph, ""), use.names=FALSE), levels=letters))
  if(is.null(corrected_char_count)) {
    corrected_char_count <- char_table
  }
  else {
    corrected_char_count <- corrected_char_count + char_table
  }
}

tess_pre_char_correct <- 0
tess_post_char_correct <- 0

for (i in names(ground_char_count)) {
  tess_pre_char_correct <- tess_pre_char_correct + min(ground_char_count[i], tess_char_count[i])
  tess_post_char_correct <- tess_post_char_correct + min(ground_char_count[i], corrected_char_count[i])
}

# Character-wise recall & precision
OCR_performance_table["character_wise_recall","Tesseract"] <- tess_pre_char_correct / sum(ground_char_count)
OCR_performance_table["character_wise_precision","Tesseract"] <- tess_pre_char_correct / sum(tess_char_count)
OCR_performance_table["character_wise_recall","Tesseract_with_postprocessing"] <- tess_post_char_correct / sum(ground_char_count)
OCR_performance_table["character_wise_precision","Tesseract_with_postprocessing"] <- tess_post_char_correct / sum(corrected_char_count)

kable(OCR_performance_table, caption="Summary of OCR performance")
```

# References {-}

1. Borovikov, E. (2014). *A survey of modern optical character recognition techniques*. arXiv preprint arXiv:1412.4183.[pdf](https://pdfs.semanticscholar.org/79c9/cc90b8c2e2c9c54c3862935ea00df7dd56ed.pdf)
(This paper is the source of our evaluation criterion)

2. Kukich, K. (1992). *Techniques for automatically correcting words in text*. Acm Computing Surveys (CSUR), 24(4), 377-439. [pdf](http://www.unige.ch/eti/ptt/docs/kukich-92.pdf)
(This paper is the benchmark review paper)